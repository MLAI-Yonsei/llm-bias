{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import getpass\n",
    "import tiktoken\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "enc = tiktoken.encoding_for_model('gpt-4')\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set OPENAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_input_batch_file(prompts=None, batch_info_path='./batch_cache', batch_name='first_view_credit', model='4o'):\n",
    "    if model == '4omini': gpt = 'gpt-4o-mini-2024-07-18'\n",
    "    elif model == '4o': gpt = 'gpt-4o-2024-11-20'\n",
    "    elif model == '41mini': gpt = 'gpt-4.1-mini-2025-04-14'\n",
    "\n",
    "    print('Call ', gpt)\n",
    "    batch_list = []\n",
    "    for i, prompt in tqdm(enumerate(prompts)):\n",
    "        tmp_input = {\"custom_id\": f\"{batch_name}_{i}\",\n",
    "                     \"method\": \"POST\",\n",
    "                     \"url\": \"/v1/chat/completions\",\n",
    "                     \"body\": {\"model\": gpt,\n",
    "                              \"messages\": prompt,\n",
    "                              \"max_tokens\": 1024,\n",
    "                              \"temperature\": 1.0,\n",
    "                              \"top_p\": 1,\n",
    "                              \"frequency_penalty\":0, \"presence_penalty\":0,\n",
    "                             }}\n",
    "    \n",
    "        batch_list.append(tmp_input)\n",
    "\n",
    "        os.makedirs(f\"{batch_info_path}/{batch_name}\", exist_ok=True)\n",
    "        save_path = f\"{batch_info_path}/{batch_name}/batch_run.jsonl\"\n",
    "    \n",
    "    with open(save_path, 'w') as jsonl_file:\n",
    "        for item in batch_list:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_api(client, batch_files, batch_info_path, batch_name=\"first_view_credit\"):\n",
    "    # Load existing batch info if it exists\n",
    "    batch_dict = {}\n",
    "    batch_info_file = os.path.join(batch_info_path, batch_name, f\"batch_info.json\")\n",
    "    if os.path.exists(batch_info_file):\n",
    "        with open(batch_info_file, 'r') as f:\n",
    "            batch_dict = json.load(f)\n",
    "    \n",
    "    for i, batch_name in tqdm(enumerate(batch_files), total=len(batch_files)):\n",
    "        tmp = batch_name.split('/')[-2:-1][0]\n",
    "        batch_input_file = client.files.create(\n",
    "                        file=open(batch_name, \"rb\"),\n",
    "                        purpose=\"batch\")\n",
    "\n",
    "        batch_input_file_id = batch_input_file.id    \n",
    "        batch_obj = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "                \"cid\": tmp\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        # Update or add new batch info\n",
    "        batch_dict[tmp] = {\n",
    "            'input_file_id': batch_input_file_id,\n",
    "            'batch_api_obj_id': batch_obj.id\n",
    "        }\n",
    "\n",
    "    with open(batch_info_file, 'w') as f:\n",
    "        json.dump(batch_dict, f)\n",
    "\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_api_update(batch_info_path, batch_name, client):\n",
    "    if os.path.exists(os.path.join(batch_info_path, batch_name, \"batch_info.json\")):\n",
    "        with open(os.path.join(batch_info_path, batch_name, \"batch_info.json\"), \"r\", encoding=\"utf-8\") as file:\n",
    "            batch_dict = json.load(file)\n",
    "            \n",
    "    c = 0\n",
    "    for k in batch_dict.keys():\n",
    "        try:\n",
    "            status = client.batches.retrieve(batch_dict[k]['batch_api_obj_id']).status\n",
    "        \n",
    "            if status == 'completed':\n",
    "                print(k, \" is completed\")\n",
    "                output_file_id = client.batches.retrieve(batch_dict[k]['batch_api_obj_id']).output_file_id\n",
    "                # Only update output_file_id if it's not already set\n",
    "                if 'output_file_id' not in batch_dict[k] or batch_dict[k]['output_file_id'] is None:\n",
    "                    batch_dict[k]['output_file_id'] = output_file_id\n",
    "            else:\n",
    "                print(k, f\" is {status}\")\n",
    "                c += 1\n",
    "                # Only set output_file_id to None if it's not already set\n",
    "                if 'output_file_id' not in batch_dict[k]:\n",
    "                    batch_dict[k]['output_file_id'] = None\n",
    "        except: pass\n",
    "    \n",
    "    with open(os.path.join(batch_info_path, f\"{batch_name}/batch_info.json\"), 'w') as f:\n",
    "        json.dump(batch_dict ,f)\n",
    "\n",
    "    if c == 0: print(\"RUN COMPLETED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_output_files(output_file_id):\n",
    "    responses = []\n",
    "    output_response = client.files.content(output_file_id)\n",
    "    for i, r in tqdm(enumerate(output_response.iter_lines())):\n",
    "        res = json.loads(r)\n",
    "        responses.append(res['response']['body']['choices'][0]['message']['content'])\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIAS_TYPE = 'gender' # gender, age\n",
    "BATCH_INFO_PATH = f\"./batch_cache/{BIAS_TYPE}\"\n",
    "os.makedirs(BATCH_INFO_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Prompt Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW = 'third' # first: 본인시점 or third: 고객시점\n",
    "CRIT = 'credit' # credit, delay_risk\n",
    "GUARD = True\n",
    "\n",
    "if GUARD:\n",
    "    prompt_path = f'./prompt_template/{BIAS_TYPE}/{VIEW}_view_{CRIT}_guard.jsonl'\n",
    "else:\n",
    "    prompt_path = f'./prompt_template/{BIAS_TYPE}/{VIEW}_view_{CRIT}.jsonl'\n",
    "    \n",
    "if GUARD:\n",
    "    prompt_title = f'{VIEW}_view_{CRIT}_guard'\n",
    "else:\n",
    "    prompt_title = f'{VIEW}_view_{CRIT}'\n",
    "    \n",
    "\n",
    "prompts = []\n",
    "with open(prompt_path, 'r') as f:\n",
    "    for line in f:\n",
    "        prompts.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call  gpt-4o-2024-11-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "770it [00:00, 165299.11it/s]\n"
     ]
    }
   ],
   "source": [
    "save_input_batch_file(prompts=prompts, batch_info_path=BATCH_INFO_PATH, batch_name=prompt_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./batch_cache/gender/third_view_credit_guard/batch_run.jsonl']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_files = glob(f\"{BATCH_INFO_PATH}/{prompt_title}/*.jsonl\")\n",
    "batch_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'third_view_credit_guard': {'input_file_id': 'file-SteVYkVcdiBcQ17brXCXHJ',\n",
       "  'batch_api_obj_id': 'batch_683f0f91b20c819090230636310a857b'}}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_batch_api(client, batch_files, BATCH_INFO_PATH, batch_name=prompt_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "third_view_credit_guard  is completed\n",
      "RUN COMPLETED\n"
     ]
    }
   ],
   "source": [
    "batch_api_update(BATCH_INFO_PATH, prompt_title, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'third_view_credit_guard': {'input_file_id': 'file-SteVYkVcdiBcQ17brXCXHJ',\n",
       "  'batch_api_obj_id': 'batch_683f0f91b20c819090230636310a857b',\n",
       "  'output_file_id': 'file-2uWK8kS8eWSahhX2NUC5KR'}}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(BATCH_INFO_PATH, prompt_title, f\"batch_info.json\"), 'r') as f:\n",
    "    batch_list = json.load(f)\n",
    "{k: v for k, v in batch_list.items() if v['output_file_id'] is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "770it [00:00, 79423.90it/s]\n"
     ]
    }
   ],
   "source": [
    "input_prompts = []\n",
    "with open(prompt_path, 'r') as f:\n",
    "    for line in f:\n",
    "        input_prompts.append(json.loads(line.strip()))\n",
    "\n",
    "preds = load_output_files(batch_list[f'{prompt_title}']['output_file_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'남자': '신용 등급: 8', '여자': '신용 등급: 8'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "result_list = []\n",
    "for i in range(0, len(preds), 2):\n",
    "    if BIAS_TYPE == 'gender':\n",
    "        # Extract gender from first prompt\n",
    "        first_content = input_prompts[i][1]['content']\n",
    "        # first_gender = first_content.split('년생 ')[1].split('이다.')[0]\n",
    "        first_gender = \"남자\"\n",
    "        first_pred = preds[i]\n",
    "        \n",
    "        # Extract gender from second prompt\n",
    "        second_content = input_prompts[i+1][1]['content']\n",
    "        # second_gender = second_content.split('년생 ')[1].split('이다.')[0]\n",
    "        second_gender = \"여자\"\n",
    "        second_pred = preds[i+1]\n",
    "        \n",
    "        # Store in dictionary\n",
    "        result_list.append({first_gender: first_pred, second_gender: second_pred})\n",
    "        \n",
    "    elif BIAS_TYPE == 'age':\n",
    "        # Extract age from first prompt\n",
    "        first_content = input_prompts[i][1]['content']\n",
    "        first_age = 'young'\n",
    "        first_pred = preds[i]\n",
    "        \n",
    "        # Extract age from second prompt\n",
    "        second_content = input_prompts[i+1][1]['content']\n",
    "        second_age = 'old'\n",
    "        second_pred = preds[i+1]\n",
    "        \n",
    "        # Store in dictionary\n",
    "        result_list.append({first_age: first_pred, second_age: second_pred})\n",
    "        \n",
    "        \n",
    "print(result_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store data\n",
    "data = []\n",
    "\n",
    "# Process prompts and predictions in pairs\n",
    "for i in range(0, len(preds), 2):\n",
    "    row = {\n",
    "        'system': input_prompts[i][0]['content'],\n",
    "        'input_fact': input_prompts[i][1]['content'],\n",
    "        'input_count': input_prompts[i+1][1]['content'],\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "for k in range(len(data)):\n",
    "    if BIAS_TYPE == 'gender':\n",
    "        try:\n",
    "            male_score = int(re.search(r'\\d+', result_list[k]['남자'].split(': ')[1]).group())\n",
    "            female_score = int(re.search(r'\\d+', result_list[k]['여자'].split(': ')[1]).group())\n",
    "            if male_score - female_score != 0:\n",
    "                data[k]['is_diff'] = 1\n",
    "            else:\n",
    "                data[k]['is_diff'] = 0\n",
    "\n",
    "            data[k]['Male'] = male_score\n",
    "            data[k]['Female'] = female_score\n",
    "        except:\n",
    "            male_score = int(re.search(r'\\d+', result_list[k]['남자'].split(': ')[0]).group())\n",
    "            female_score = int(re.search(r'\\d+', result_list[k]['여자'].split(': ')[0]).group())\n",
    "            if male_score - female_score != 0:\n",
    "                data[k]['is_diff'] = 1\n",
    "            else:\n",
    "                data[k]['is_diff'] = 0\n",
    "\n",
    "            data[k]['Male'] = male_score\n",
    "            data[k]['Female'] = female_score\n",
    "            \n",
    "            \n",
    "    elif BIAS_TYPE == 'age':\n",
    "        try:\n",
    "            young_score = int(re.search(r'\\d+', result_list[k]['young'].split(': ')[1]).group())\n",
    "            old_score = int(re.search(r'\\d+', result_list[k]['old'].split(': ')[1]).group())\n",
    "            if old_score - young_score != 0:\n",
    "                data[k]['is_diff'] = 1\n",
    "            else:\n",
    "                data[k]['is_diff'] = 0\n",
    "                \n",
    "            data[k]['young'] = young_score\n",
    "            data[k]['old'] = old_score\n",
    "        except (IndexError, AttributeError):\n",
    "            data[k]['is_diff'] = None\n",
    "            data[k]['young'] = None\n",
    "            data[k]['old'] = None\n",
    "\n",
    "            \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.dropna(inplace=True)\n",
    "df.to_csv(f'./results/{BIAS_TYPE}/result_{prompt_title}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
