{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import getpass\n",
    "import tiktoken\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "enc = tiktoken.encoding_for_model('gpt-4')\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_input_batch_file(prompts=None, batch_name=None, model='41'):\n",
    "    if model == '4omini': gpt = 'gpt-4o-mini-2024-07-18'\n",
    "    elif model == '4o': gpt = 'gpt-4o-2024-11-20'\n",
    "    elif model == '41mini': gpt = 'gpt-4.1-mini-2025-04-14'\n",
    "    elif model == '41': gpt = 'gpt-4.1-2025-04-14'\n",
    "\n",
    "    print('Call ', gpt)\n",
    "    k = 0\n",
    "    batch_list = []\n",
    "    for i, prompt in tqdm(enumerate(prompts)):\n",
    "        tmp_input = {\"custom_id\": f\"{batch_name}_{i}\",\n",
    "                     \"method\": \"POST\",\n",
    "                     \"url\": \"/v1/chat/completions\",\n",
    "                     \"body\": {\"model\": gpt,\n",
    "                              \"messages\": prompt,\n",
    "                              \"max_tokens\": 1024,\n",
    "                              \"temperature\": 1.0,\n",
    "                              \"top_p\": 1,\n",
    "                              \"frequency_penalty\":0, \"presence_penalty\":0,\n",
    "                             }}\n",
    "    \n",
    "        batch_list.append(tmp_input)\n",
    "    \n",
    "        if len(batch_list) >= 40000:\n",
    "            with open(f\"./finance-legal-mrc/{batch_name}_{k}.jsonl\", 'w') as jsonl_file:\n",
    "                for item in batch_list:\n",
    "                    jsonl_file.write(json.dumps(item) + '\\n')\n",
    "            k += 1\n",
    "            batch_list = []\n",
    "    \n",
    "    with open(f\"./finance-legal-mrc/{batch_name}_{k}.jsonl\", 'w') as jsonl_file:\n",
    "        for item in batch_list:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_api(client, batch_files, batch_info_path):\n",
    "    # Load existing batch info if it exists\n",
    "    batch_dict = {}\n",
    "    batch_info_file = os.path.join(batch_info_path, \"batch_info.json\")\n",
    "    if os.path.exists(batch_info_file):\n",
    "        with open(batch_info_file, 'r') as f:\n",
    "            batch_dict = json.load(f)\n",
    "    \n",
    "    for i, batch_name in tqdm(enumerate(batch_files), total=len(batch_files)):\n",
    "        tmp = batch_name.split(\"/\")[-1].split(\".\")[0]\n",
    "        batch_input_file = client.files.create(\n",
    "                        file=open(batch_name, \"rb\"),\n",
    "                        purpose=\"batch\")\n",
    "\n",
    "        batch_input_file_id = batch_input_file.id    \n",
    "        batch_obj = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "                \"cid\": tmp\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        # Update or add new batch info\n",
    "        batch_dict[tmp] = {\n",
    "            'input_file_id': batch_input_file_id,\n",
    "            'batch_api_obj_id': batch_obj.id\n",
    "        }\n",
    "\n",
    "    with open(batch_info_file, 'w') as f:\n",
    "        json.dump(batch_dict, f)\n",
    "\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_api_update(batch_info_path, client):\n",
    "    if os.path.exists(os.path.join(batch_info_path, \"batch_info.json\")):\n",
    "        with open(os.path.join(batch_info_path, \"batch_info.json\"), \"r\", encoding=\"utf-8\") as file:\n",
    "            batch_dict = json.load(file)\n",
    "            \n",
    "    c = 0\n",
    "    for k in batch_dict.keys():\n",
    "        try:\n",
    "            status = client.batches.retrieve(batch_dict[k]['batch_api_obj_id']).status\n",
    "        \n",
    "            if status == 'completed':\n",
    "                print(k, \" is completed\")\n",
    "                output_file_id = client.batches.retrieve(batch_dict[k]['batch_api_obj_id']).output_file_id\n",
    "                # Only update output_file_id if it's not already set\n",
    "                if 'output_file_id' not in batch_dict[k] or batch_dict[k]['output_file_id'] is None:\n",
    "                    batch_dict[k]['output_file_id'] = output_file_id\n",
    "            else:\n",
    "                print(k, f\" is {status}\")\n",
    "                c += 1\n",
    "                # Only set output_file_id to None if it's not already set\n",
    "                if 'output_file_id' not in batch_dict[k]:\n",
    "                    batch_dict[k]['output_file_id'] = None\n",
    "        except: pass\n",
    "    \n",
    "    with open(os.path.join(batch_info_path, \"batch_info.json\"), 'w') as f:\n",
    "        json.dump(batch_dict ,f)\n",
    "\n",
    "    if c == 0: print(\"RUN COMPLTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = f'./finfairnessQA_prompt.jsonl'\n",
    "prompts = []\n",
    "with open(prompt_path, 'r') as f:\n",
    "    for line in f:\n",
    "        prompts.append(json.loads(line.strip()))\n",
    "prompts = prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = f'./finfairnessQA_prompt_w_g.jsonl'\n",
    "prompts_w_g = []\n",
    "with open(prompt_path, 'r') as f:\n",
    "    for line in f:\n",
    "        prompts_w_g.append(json.loads(line.strip()))\n",
    "prompts_w_g = prompts_w_g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call  gpt-4o-2024-11-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 476084.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call  gpt-4o-2024-11-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 557012.48it/s]\n"
     ]
    }
   ],
   "source": [
    "save_input_batch_file(prompts=prompts, batch_name=f'finfairnessqa_task', model='4o')\n",
    "save_input_batch_file(prompts=prompts_w_g, batch_name=f'finfairnessqa_task_w_g', model='4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./finance-legal-mrc/finfairnessqa_task_w_g_0.jsonl', './finance-legal-mrc/finfairnessqa_task_0.jsonl']\n"
     ]
    }
   ],
   "source": [
    "batch_files = glob(f\"./finance-legal-mrc/*finfairnessqa_task*.jsonl\")\n",
    "print(batch_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.55s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'finfairnessqa_task_w_g_0': {'input_file_id': 'file-FUszKbCnrR6ayEv3NU2svB',\n",
       "  'batch_api_obj_id': 'batch_68430ca646988190bef5f0284ee445cc'},\n",
       " 'finfairnessqa_task_0': {'input_file_id': 'file-XTDLb9zwZokCQuQEQzAk1h',\n",
       "  'batch_api_obj_id': 'batch_68430ca7739c8190839a23fa85e53440'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_info_path = \"./finance-legal-mrc\"\n",
    "run_batch_api(client, batch_files, batch_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finfairnessqa_task_w_g_0  is in_progress\n",
      "finfairnessqa_task_0  is in_progress\n"
     ]
    }
   ],
   "source": [
    "batch_api_update(batch_info_path, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_output_files(output_file_id):\n",
    "    responses = []\n",
    "    output_response = client.files.content(output_file_id)\n",
    "    for i, r in tqdm(enumerate(output_response.iter_lines())):\n",
    "        res = json.loads(r)\n",
    "        responses.append(res['response']['body']['choices'][0]['message']['content'])\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finfairnessqa_task_w_g_0  is completed\n",
      "finfairnessqa_task_0  is completed\n",
      "RUN COMPLTED\n"
     ]
    }
   ],
   "source": [
    "batch_info_path = \"./finance-legal-mrc\"\n",
    "batch_api_update(batch_info_path, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finfairnessqa_task_w_g_0': {'input_file_id': 'file-FUszKbCnrR6ayEv3NU2svB',\n",
       "  'batch_api_obj_id': 'batch_68430ca646988190bef5f0284ee445cc',\n",
       "  'output_file_id': 'file-H2yJLg6G46VYsnaxKu6DCj'},\n",
       " 'finfairnessqa_task_0': {'input_file_id': 'file-XTDLb9zwZokCQuQEQzAk1h',\n",
       "  'batch_api_obj_id': 'batch_68430ca7739c8190839a23fa85e53440',\n",
       "  'output_file_id': 'file-964uctD1EAX3jgAe2JJo5N'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(batch_info_path, \"batch_info.json\"), 'r') as f:\n",
    "    batch_list = json.load(f)\n",
    "{k: v for k, v in batch_list.items() if v['output_file_id'] is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 26119.72it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_title = 'finfairnessqa_task'\n",
    "prompt_path = f'./finfairnessQA_prompt.jsonl'\n",
    "input_prompts = []\n",
    "with open(prompt_path, 'r') as f:\n",
    "    for line in f:\n",
    "        input_prompts.append(json.loads(line.strip()))\n",
    "\n",
    "preds = load_output_files(batch_list[f'{prompt_title}_0']['output_file_id'])\n",
    "preds = ['답변거부' if p not in ['참', '거짓'] else p for p in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA task result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./bias_qa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['response'] = preds\n",
    "df['Acc'] = (df['정답'] == df['response']).astype(int)\n",
    "accuracy = (df['정답'] == df['response']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3500\n",
      "Financial QA Accuracy: 0.5645\n",
      "Bias QA Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# f- : 금융분야 지식 QA\n",
    "f_accuracy = (df[df.Index.str.startswith('f-')]['정답'] == df[df.Index.str.startswith('f-')]['response']).mean()\n",
    "\n",
    "print(f\"Financial QA Accuracy: {f_accuracy:.4f}\")\n",
    "\n",
    "# b- : 편향성 QA\n",
    "b_accuracy = (df[df.Index.str.startswith('b-')]['정답'] == df[df.Index.str.startswith('b-')]['response']).mean()\n",
    "print(f\"Bias QA Accuracy: {b_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
