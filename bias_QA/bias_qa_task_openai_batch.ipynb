{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import getpass\n",
    "import tiktoken\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "enc = tiktoken.encoding_for_model('gpt-4')\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_input_batch_file(prompts=None, batch_name=None, model='41'):\n",
    "    if model == '4omini': gpt = 'gpt-4o-mini-2024-07-18'\n",
    "    elif model == '4o': gpt = 'gpt-4o-2024-11-20'\n",
    "    elif model == '41mini': gpt = 'gpt-4.1-mini-2025-04-14'\n",
    "    elif model == '41': gpt = 'gpt-4.1-2025-04-14'\n",
    "\n",
    "    print('Call ', gpt)\n",
    "    k = 0\n",
    "    batch_list = []\n",
    "    for i, prompt in tqdm(enumerate(prompts)):\n",
    "        tmp_input = {\"custom_id\": f\"{batch_name}_{i}\",\n",
    "                     \"method\": \"POST\",\n",
    "                     \"url\": \"/v1/chat/completions\",\n",
    "                     \"body\": {\"model\": gpt,\n",
    "                              \"messages\": prompt,\n",
    "                              \"max_tokens\": 1024,\n",
    "                              \"temperature\": 1.0,\n",
    "                              \"top_p\": 1,\n",
    "                              \"frequency_penalty\":0, \"presence_penalty\":0,\n",
    "                             }}\n",
    "    \n",
    "        batch_list.append(tmp_input)\n",
    "    \n",
    "        if len(batch_list) >= 40000:\n",
    "            with open(f\"./finance-legal-mrc/{batch_name}_{k}.jsonl\", 'w') as jsonl_file:\n",
    "                for item in batch_list:\n",
    "                    jsonl_file.write(json.dumps(item) + '\\n')\n",
    "            k += 1\n",
    "            batch_list = []\n",
    "    \n",
    "    with open(f\"./finance-legal-mrc/{batch_name}_{k}.jsonl\", 'w') as jsonl_file:\n",
    "        for item in batch_list:\n",
    "            jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_api(client, batch_files, batch_info_path):\n",
    "    # Load existing batch info if it exists\n",
    "    batch_dict = {}\n",
    "    batch_info_file = os.path.join(batch_info_path, \"batch_info.json\")\n",
    "    if os.path.exists(batch_info_file):\n",
    "        with open(batch_info_file, 'r') as f:\n",
    "            batch_dict = json.load(f)\n",
    "    \n",
    "    for i, batch_name in tqdm(enumerate(batch_files), total=len(batch_files)):\n",
    "        tmp = batch_name.split(\"/\")[-1].split(\".\")[0]\n",
    "        batch_input_file = client.files.create(\n",
    "                        file=open(batch_name, \"rb\"),\n",
    "                        purpose=\"batch\")\n",
    "\n",
    "        batch_input_file_id = batch_input_file.id    \n",
    "        batch_obj = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "                \"cid\": tmp\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        # Update or add new batch info\n",
    "        batch_dict[tmp] = {\n",
    "            'input_file_id': batch_input_file_id,\n",
    "            'batch_api_obj_id': batch_obj.id\n",
    "        }\n",
    "\n",
    "    with open(batch_info_file, 'w') as f:\n",
    "        json.dump(batch_dict, f)\n",
    "\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_api_update(batch_info_path, client):\n",
    "    if os.path.exists(os.path.join(batch_info_path, \"batch_info.json\")):\n",
    "        with open(os.path.join(batch_info_path, \"batch_info.json\"), \"r\", encoding=\"utf-8\") as file:\n",
    "            batch_dict = json.load(file)\n",
    "            \n",
    "    c = 0\n",
    "    for k in batch_dict.keys():\n",
    "        try:\n",
    "            status = client.batches.retrieve(batch_dict[k]['batch_api_obj_id']).status\n",
    "        \n",
    "            if status == 'completed':\n",
    "                print(k, \" is completed\")\n",
    "                output_file_id = client.batches.retrieve(batch_dict[k]['batch_api_obj_id']).output_file_id\n",
    "                # Only update output_file_id if it's not already set\n",
    "                if 'output_file_id' not in batch_dict[k] or batch_dict[k]['output_file_id'] is None:\n",
    "                    batch_dict[k]['output_file_id'] = output_file_id\n",
    "            else:\n",
    "                print(k, f\" is {status}\")\n",
    "                c += 1\n",
    "                # Only set output_file_id to None if it's not already set\n",
    "                if 'output_file_id' not in batch_dict[k]:\n",
    "                    batch_dict[k]['output_file_id'] = None\n",
    "        except: pass\n",
    "    \n",
    "    with open(os.path.join(batch_info_path, \"batch_info.json\"), 'w') as f:\n",
    "        json.dump(batch_dict ,f)\n",
    "\n",
    "    if c == 0: print(\"RUN COMPLTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = f'./finfairnessQAgen_prompt.jsonl'\n",
    "prompts = []\n",
    "with open(prompt_path, 'r') as f:\n",
    "    for line in f:\n",
    "        prompts.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call  gpt-4.1-2025-04-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3582it [00:00, 842293.94it/s]\n"
     ]
    }
   ],
   "source": [
    "save_input_batch_file(prompts=prompts, batch_name=f'finfairnessqa_sampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./finance-legal-mrc/finfairnessqa_sampled_0.jsonl']\n"
     ]
    }
   ],
   "source": [
    "batch_files = glob(f\"./finance-legal-mrc/*finfairnessqa_sampled*.jsonl\")\n",
    "print(batch_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'finfairnessqa_sampled_0': {'input_file_id': 'file-F8SD9VPNKsGAc6RsZNGpoV',\n",
       "  'batch_api_obj_id': 'batch_683e848261688190aecd345435afd51f'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_info_path = \"./finance-legal-mrc\"\n",
    "run_batch_api(client, batch_files, batch_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finfairnessqa_sampled_0  is validating\n"
     ]
    }
   ],
   "source": [
    "batch_api_update(batch_info_path, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finfairnessqa_task_w_g_0  is completed\n",
      "finfairnessqa_task_0  is completed\n",
      "RUN COMPLTED\n"
     ]
    }
   ],
   "source": [
    "batch_info_path = \"./finance-legal-mrc\"\n",
    "batch_api_update(batch_info_path, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_output_files(output_file_id):\n",
    "    responses = []\n",
    "    output_response = client.files.content(output_file_id)\n",
    "    for i, r in tqdm(enumerate(output_response.iter_lines())):\n",
    "        res = json.loads(r)\n",
    "        responses.append(res['response']['body']['choices'][0]['message']['content'])\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finfairnessqa_task_w_g_0': {'input_file_id': 'file-FUszKbCnrR6ayEv3NU2svB',\n",
       "  'batch_api_obj_id': 'batch_68430ca646988190bef5f0284ee445cc',\n",
       "  'output_file_id': 'file-H2yJLg6G46VYsnaxKu6DCj'},\n",
       " 'finfairnessqa_task_0': {'input_file_id': 'file-XTDLb9zwZokCQuQEQzAk1h',\n",
       "  'batch_api_obj_id': 'batch_68430ca7739c8190839a23fa85e53440',\n",
       "  'output_file_id': 'file-964uctD1EAX3jgAe2JJo5N'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(batch_info_path, \"batch_info.json\"), 'r') as f:\n",
    "    batch_list = json.load(f)\n",
    "{k: v for k, v in batch_list.items() if v['output_file_id'] is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3582it [00:00, 61353.74it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_title = 'finfairnessqa_sampled'\n",
    "prompt_path = f'./finfairnessQAgen_prompt.jsonl'\n",
    "input_prompts = []\n",
    "with open(prompt_path, 'r') as f:\n",
    "    for line in f:\n",
    "        input_prompts.append(json.loads(line.strip()))\n",
    "\n",
    "preds = load_output_files(batch_list[f'{prompt_title}_0']['output_file_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "preds_new = []\n",
    "with open('generted_qa.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, pred in enumerate(preds, 1):\n",
    "        if len(pred.split(\"질문:\")) >= 3:\n",
    "            for p in pred.split(\"질문:\"):\n",
    "                if len(p) == 0: continue\n",
    "                f.write(f\"{count}. 질문: {p}\\n=====\\n\")\n",
    "                count += 1\n",
    "                preds_new.append(f'질문: {p}')\n",
    "        else:\n",
    "            f.write(f\"{count}. {pred}\\n=====\\n\")\n",
    "            count += 1\n",
    "            preds_new.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "gened_qa = {}\n",
    "for i, res in enumerate(preds_new):\n",
    "    gened_qa[i] = res\n",
    "\n",
    "# Save preds_new to pickle file\n",
    "with open('gened_qa.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(gened_qa, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_qa = []\n",
    "for res in preds_new:\n",
    "    tmp = res.split(\"\\n답변\")[0]\n",
    "    tmp = tmp.replace(\"  \", \" \")\n",
    "    tmp = tmp.replace(\". \",\".\")\n",
    "    selected_qa.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_qa\n",
    "selected_qa_dict = {}\n",
    "for i, q in enumerate(selected_qa):\n",
    "    selected_qa_dict[i] = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('selected_qa_dict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(selected_qa_dict, f, ensure_ascii=False)\n",
    "\n",
    "## selected_qa_dict.json은 gpt_api.py에서 LLM에게 입력되어 특정 집단에 대한 편향성을 확인하기에 적절한 질문들만 선별하는 과정을 거침\n",
    "## 결과는 biasQA_filtered_qa.json에 저장됨 => load_hf_dataset_n_query_prompt_gen.ipynb에서 불러와짐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
